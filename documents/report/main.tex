\documentclass[conference,11pt]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{bookmark}
\usepackage{stfloats}
\usepackage{float}

\begin{document}

\title{Comparative Analysis of DCGANs and Diffusion Models for MNIST Generation: Training Stability, Mode Collapse, and Computational Tradeoffs}

\author{\IEEEauthorblockN{Michael Johnson}
\IEEEauthorblockA{\textit{ECE5570--Machine Learning at Scale} \\
	\textit{Assignment 4}\\
December 9, 2025}}

\maketitle

\begin{abstract}
This study compares Deep Convolutional Generative Adversarial Networks (DCGANs) and diffusion models for MNIST digit generation and denoising. Two DCGAN configurations are evaluated: a baseline with latent dimension 256, depth 3, and no dropout, and a modified version with latent dimension 64, depth 4, and dropout 0.3. The baseline produces diverse, high-quality samples while the modified configuration exhibits severe mode collapse. Results reveal fundamental tradeoffs between model families. GANs offer fast inference (1.29ms/sample) but suffer from training instability. Diffusion models provide stable training and consistent quality but require 115--230 times longer inference (153--297ms/sample for 100 denoising steps). These findings validate theoretical predictions and provide insights for model selection in production environments.
\end{abstract}

\begin{IEEEkeywords}
Generative Adversarial Networks, DCGAN, Diffusion Models, MNIST, Mode Collapse, Deep Learning, Image Generation, Comparative Analysis
\end{IEEEkeywords}

% -------------------
\section{Introduction and Objectives}
\subsection{Motivation for Image Generation}
Image generation enables applications in data augmentation, creative AI, and simulation. Generative models synthesize realistic data and advance understanding of high-dimensional distributions.

\subsection{Brief Overview of GANs and Diffusion Models}
Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} consist of a generator and discriminator competing in a minimax game. DCGANs \cite{radford2016unsupervised} apply convolutional architectures to improve image synthesis. Diffusion models generate data by iteratively denoising random noise, offering stable training and high sample quality.

% -------------------
\section{Methods}
\subsection{Dataset and Preprocessing}
Experiments use the MNIST dataset of 60,000 handwritten digit images (28×28 grayscale). Images are normalized to $[-1, 1]$ to match the generator's Tanh output.

\subsection{DCGAN Architecture and Training Regime}
Figure~\ref{fig:gan_architecture} illustrates the complete DCGAN architecture. The generator maps a latent vector $z \sim \mathcal{N}(0,I)$ to an image using transposed convolutions, batch normalization, and ReLU activations with Tanh at the output. The discriminator uses strided convolutions, batch normalization, LeakyReLU, and adaptive pooling to produce a $1\times1$ output. Dropout provides optional regularization. Training alternates between discriminator updates (classifying real vs. fake) and generator updates (fooling the discriminator) using binary cross-entropy loss. Fixed random seeds and deterministic cuDNN settings ensure reproducibility.

\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{GAN_Architecture.pdf}
\caption{DCGAN architecture showing the generator network (latent noise → 256×7×7 → 128×14×14 → 64×28×28 → 1×28×28 output) and discriminator network (1×28×28 input → 64×14×14 → 128×7×7 → 256×3×3 → Real/Fake classification). Real MNIST training images and fake generated images are both processed by the discriminator during adversarial training.}
\label{fig:gan_architecture}
\end{figure*}

\subsection{Diffusion Model Configuration}
A pretrained Denoising Diffusion Probabilistic Model (DDPM) from HuggingFace's Diffusers library (1aurent/ddpm-mnist) provides comparison. The model uses a UNet backbone with timestep conditioning, trained on MNIST digits at 28×28 resolution. Gaussian noise at three levels ($\sigma = 0.3, 0.5, 0.7$) is added to MNIST images to demonstrate forward diffusion. The model then performs iterative denoising over 100 steps using the DDPM scheduler.

\subsection{Cluster, Container, and Slurm Setup}
Experiments run on the AI-Panther HPC cluster with SLURM job scheduling. Apptainer containers ensure consistent environments (Python 3.11, PyTorch, CUDA). SLURM scripts specify resource allocation (GPU, CPUs, memory) and experiment parameters.

% -------------------
\section{Results}

\subsection{Visual Examples from Both Models}

\subsubsection{Baseline Configuration (latent\_dim=256, depth=3, dropout=0.0)}
Figure~\ref{fig:baseline_samples} shows generated sample evolution across epochs. The baseline model improves progressively:
\begin{itemize}
    \item \textbf{Epoch 1:} Initial samples already show recognizable digit-like shapes with clear structure, though blurry and imperfect. The larger latent dimension (256) and balanced architecture enable the generator to produce meaningful outputs from the start.
    \item \textbf{Epochs 5--10:} Digit-like shapes sharpen rapidly. By epoch 10, samples show clear digit contours with varied forms, though blurriness persists.
    \item \textbf{Epochs 15--30:} Quality improves substantially. All digit classes (0--9) appear with increasing frequency and clarity. Samples show diversity in stroke thickness, orientation, and style.
    \item \textbf{Epochs 35--50:} Quality plateaus with continued refinement. Digits exhibit sharp edges, natural handwriting variations, and strong diversity. No repeated or nearly identical samples appear, indicating successful learning of the full MNIST distribution.
\end{itemize}

The baseline configuration avoids mode collapse throughout all 50 epochs, maintaining diverse, high-quality digit generation.

\begin{figure}
\centering
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_001.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_050.png}
\caption{Baseline model samples at epochs 1 (left) and 50 (right), showing clear progression from noise to diverse, high-quality digits.}
\label{fig:baseline_samples}
\end{figure}

\subsubsection{Modified Configuration (latent\_dim=64, depth=4, dropout=0.3)}
Figure~\ref{fig:modified_samples} reveals training failure:
\begin{itemize}
    \item \textbf{Epoch 1:} Initial samples show mostly amorphous noise with minimal digit structure, contrasting sharply with the baseline.
    \item \textbf{Epochs 5--10:} Grid-like artifacts and checkerboard patterns emerge. No recognizable digits form.
    \item \textbf{Epochs 10--20:} Sample quality degrades further. Generated images become uniform, dominated by repetitive grid textures and high-frequency noise.
    \item \textbf{Epochs 20--30:} Mode collapse accelerates. By epoch 30, samples converge to nearly identical outputs featuring regular grid patterns with no semantic content.
    \item \textbf{Epochs 35--50:} Complete mode collapse solidifies. All 64 samples in each grid are visually indistinguishable, showing identical grid/checkerboard artifacts. The generator produces a single meaningless pattern repeatedly, failing to represent the MNIST digit distribution.
\end{itemize}

The modified model exhibits textbook mode collapse. The generator finds a single output that consistently fools the discriminator, abandoning the goal of learning the data distribution.

\begin{figure}
\centering
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_001.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_050.png}
\caption{Modified model samples at epochs 1 (left) and 50 (right), showing mode collapse to identical grid patterns.}
\label{fig:modified_samples}
\end{figure}

\subsection{Quantitative Metrics or Proxy Measures}

Table~\ref{tab:metrics} summarizes quantitative metrics:

\begin{table}
\centering
\caption{Training Metrics Comparison}
\label{tab:metrics}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Modified} \\
\midrule
Latent Dimension & 256 & 64 \\
Network Depth & 3 & 4 \\
Dropout & 0.0 & 0.3 \\
\midrule
Final Gen Loss & 1.75 & 10.40 \\
Final Disc Loss & 0.65 & 0.0001 \\
Min Generator Loss & 0.49 & 0.68 \\
Min Discriminator Loss & 0.35 & 0.00 \\
\midrule
Training Time (min) & 7.49 & 12.22 \\
Time/Epoch (sec) & 8.98 & 14.66 \\
Inference Time (ms) & 1.29 & 0.70 \\
\midrule
GPU Utilization (\%) & 100 & 100 \\
GPU Memory (MB) & 12631 & 12631 \\
Memory Utilization (\%) & 28--31 & 28--31 \\
\midrule
Inter-sample Variance & 0.226 & 0.082 \\
Mean Pairwise Distance & 0.451 & 0.154 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Loss Behavior Analysis:}
\begin{itemize}
    \item \textbf{Baseline:} The final generator loss of 1.75 and discriminator loss of 0.65 show that both networks are learning effectively and remain competitive. The discriminator is slightly better than random guessing (about 66\% accuracy), and the generator continues to improve. This indicates stable and successful GAN training.

    \item \textbf{Modified:} The final generator loss of 10.40 (almost six times higher than the baseline) and discriminator loss close to 0.0001 indicate a complete breakdown in training. The discriminator classifies samples almost perfectly (close to 100\% accuracy), while the generator fails to produce realistic outputs. This sharp loss difference is a clear sign of mode collapse.
\end{itemize}

\textbf{Diversity Metrics Analysis:}
\begin{itemize}
    \item \textbf{Inter-sample Variance:} Baseline (0.226) vs Modified (0.082) shows 63.8\% reduction in sample diversity, providing direct quantitative evidence of mode collapse. Lower variance indicates samples cluster in a narrow region of the output space.
    
    \item \textbf{Mean Pairwise Distance:} Baseline (0.451) vs Modified (0.154) shows 65.8\% reduction in inter-sample distance. Samples are nearly identical, characteristic of a generator producing limited distinct outputs regardless of input noise.
\end{itemize}

\begin{figure*}
\centering
\includegraphics[width=0.48\textwidth]{../../output/dcgan_baseline/training_losses.png}
\includegraphics[width=0.48\textwidth]{../../output/dcgan_modified/training_losses.png}
\caption{Training loss curves for the baseline (left) and modified (right) models. The baseline model maintains stable, oscillating losses, while the modified model shows rapid loss divergence and instability.}
\label{fig:loss_curves}
\end{figure*}

Figure~\ref{fig:loss_curves} shows loss evolution over 23,450 iterations:
\begin{itemize}
    \item \textbf{Baseline:} Both losses stabilize after $\sim$5,000 iterations, oscillating around steady values (G: 2.0--2.7, D: 0.5--1.2). This oscillatory behavior reflects the minimax game dynamics, with neither network dominating—a sign of healthy training.
    
    \item \textbf{Modified:} Generator loss increases monotonically from $\sim$3 (iteration 0) to $>$10 (by iteration 10,000), continuing to diverge through iteration 23,450. Discriminator loss collapses rapidly to near-zero by iteration 5,000 and remains there. This one-sided optimization failure confirms that the discriminator completely dominates, rejecting all generator outputs with certainty.
\end{itemize}

The loss curves clearly illustrate the difference between the two models: the baseline maintains stable adversarial training, while the modified configuration shows severe instability and divergence.

\subsection{Training and Inference Performance}

\textbf{Computational Efficiency:}
\begin{itemize}
    \item \textbf{Baseline:} Completed 50 epochs in 7.49 minutes (8.98 seconds/epoch) on a single GPU with the larger latent dimension (256). GPU utilization maintained 100\% throughout training with consistent memory usage of 12.6 GB (28--31\% of available 40GB GPU memory).
    
    \item \textbf{Modified:} Required 12.22 minutes (14.66 seconds/epoch), representing a 63\% increase in training time. This overhead stems from the deeper architecture (4 vs. 3 layers in both G and D) and dropout regularization (0.3), which adds computational cost without improving results. Despite the longer runtime, GPU utilization remained at 100\% with identical memory footprint (12.6 GB), indicating the slowdown is due to increased FLOPs rather than resource contention.
\end{itemize}

\textbf{Generation Speed:}
\begin{itemize}
    \item \textbf{Baseline:} Average inference time of 1.29ms per sample enables real-time generation applications.
    
    \item \textbf{Modified:} Faster inference at 0.70ms per sample (46\% reduction) due to the smaller latent dimension (64 vs. 256), requiring fewer computations in the initial generator layers. However, this speed advantage is meaningless given the model's complete failure to generate valid samples.
\end{itemize}

\textbf{Efficiency-Quality Tradeoff:} The modified model demonstrates a critical failure mode: architectural changes that reduce inference latency but sacrifice training stability result in worthless outputs. The baseline model's slightly longer inference time represents a negligible cost for successful learning. Computational efficiency metrics are secondary to training success in generative models.

\subsection{Diffusion Model Denoising Results}

Figure~\ref{fig:diffusion_denoising} shows the diffusion model's denoising behavior at three noise levels ($\sigma = 0.3, 0.5, 0.7$). Each subplot displays three rows: original clean MNIST images (top), images with added Gaussian noise (middle), and denoised outputs after 100 inference steps (bottom).

\begin{figure*}
\centering
\includegraphics[width=0.32\textwidth]{../../output/diffusion_denoise/denoise_noise_0.3.png}
\includegraphics[width=0.32\textwidth]{../../output/diffusion_denoise/denoise_noise_0.5.png}
\includegraphics[width=0.32\textwidth]{../../output/diffusion_denoise/denoise_noise_0.7.png}
\caption{Diffusion model denoising at three noise levels: $\sigma = 0.3$ (left), $0.5$ (center), $0.7$ (right). Top row: original clean images. Middle row: noisy images after forward diffusion. Bottom row: denoised outputs. The model generates plausible MNIST digits but does not reconstruct originals, demonstrating generative rather than reconstructive behavior.}
\label{fig:diffusion_denoising}
\end{figure*}

\textbf{Visual Observations:}
\begin{itemize}
    \item \textbf{Low Noise ($\sigma = 0.3$):} Original digits remain clearly visible in noisy images. However, denoised outputs show completely different digit classes. The model generates clean, well-formed handwritten digits without reconstructing the originals.
    
    \item \textbf{Medium Noise ($\sigma = 0.5$):} Noise obscures more detail, but original structure remains recognizable. Denoised outputs continue showing different digit classes, confirming generative rather than reconstructive behavior.
    
    \item \textbf{High Noise ($\sigma = 0.7$):} Substantial noise degrades original images, though digit shapes remain partially visible. The model generates clean digits from the learned distribution with no correspondence to originals. All three noise levels demonstrate consistent generative behavior.
\end{itemize}

Table~\ref{tab:diffusion_metrics} presents quantitative denoising metrics:

\begin{table}
\centering
\caption{Diffusion Model Denoising Metrics}
\label{tab:diffusion_metrics}
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{$\sigma = 0.3$} & \textbf{$\sigma = 0.5$} & \textbf{$\sigma = 0.7$} \\
\midrule
MSE & 18,888 & 18,985 & 19,414 \\
PSNR (dB) & 5.38 & 5.36 & 5.27 \\
Correlation & 0.369 & 0.362 & 0.347 \\
Time/Sample (ms) & 297 & 153 & 155 \\
Inference Steps & 100 & 100 & 100 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Metric Interpretation:}
\begin{itemize}
    \item \textbf{MSE (18,888--19,414):} Extremely high values indicate large pixel-level differences between originals and denoised outputs. Standard image reconstruction typically achieves MSE $<$1000.
    
    \item \textbf{PSNR (5.27--5.38 dB):} Very low PSNR confirms poor reconstruction quality. Typical reconstruction tasks achieve PSNR $>$30 dB. Values below 20 dB indicate severe degradation.
    
    \item \textbf{Correlation (0.347--0.369):} Weak positive correlation shows denoised outputs bear little pixel-wise similarity to originals. Standard reconstruction yields correlation $>$0.9.
\end{itemize}

These metrics confirm the diffusion model performs \textit{generation} rather than \textit{reconstruction}. It samples plausible MNIST digits from the learned distribution conditioned on noisy input, rather than attempting pixel-perfect recovery. This behavior is expected for generative diffusion models trained to model $p(x)$ rather than learn a deterministic mapping $f: x_{noisy} \rightarrow x_{clean}$.

\textbf{Inference Time Analysis:} The first denoising experiment ($\sigma = 0.3$) shows 297ms per sample due to model loading and GPU initialization overhead. Subsequent experiments ($\sigma = 0.5, 0.7$) benefit from cached operations, reducing time to 153--155ms per sample. All experiments require 100 sequential denoising steps, making diffusion 115--230$\times$ slower than DCGAN's single-pass generation (1.29ms).

% -------------------
\section{Discussion}

\subsection{Mode Collapse, Stability, and Sample Diversity}

The modified DCGAN exhibits severe mode collapse. By epoch 35, all 64 generated samples become visually identical, showing only checkerboard patterns rather than recognizable digits. This persists through epoch 50 without recovery. In contrast, the baseline model produces diverse samples covering all digit classes (0--9) with varied stroke thickness, orientation, and style.

Loss curves reveal the mechanism of collapse. The baseline maintains balanced losses (G: 2.0--2.7, D: 0.5--1.2) with healthy oscillation, indicating both networks compete effectively. The modified model shows extreme divergence: generator loss climbs from 0.85 to over 10, while discriminator loss drops near zero. This indicates the discriminator perfectly rejects all generator outputs, yet the generator cannot escape its collapsed state.

Three architectural factors combine to cause collapse. The reduced latent dimension (64 vs. 256) constrains generator capacity from the start, visible even at epoch 1 where modified samples show amorphous noise while baseline samples already exhibit recognizable digit structure. The increased depth (4 vs. 3 layers) strengthens the discriminator disproportionately, and dropout (0.3) further slows generator learning. These create a power imbalance where a weak generator faces an overpowered discriminator. The discriminator achieves near-perfect classification ($D(x) \approx 1$, $D(G(z)) \approx 0$), causing vanishing gradients for the generator. GANs lack convergence guarantees, so once collapse occurs, training cannot recover naturally.

Sample diversity metrics confirm visual observations. The baseline shows inter-sample variance of 0.226 and mean pairwise distance of 0.451. The modified model shows 0.082 and 0.154 respectively, representing 63--66\% reduction in diversity.

\subsection{Diffusion Model Behavior}

The diffusion model exhibits fundamentally different behavior from GANs. Rather than generating images from random latent vectors, it removes noise through iterative denoising. At low noise ($\sigma = 0.3$), denoised outputs resemble but do not exactly match original images. At high noise ($\sigma = 0.7$), outputs diverge more from originals while remaining plausible MNIST digits.

This reflects the generative nature of diffusion models. They sample from the learned distribution conditioned on noisy input rather than performing deterministic reconstruction. Pixel-level metrics confirm this: MSE of 18,888--19,414, PSNR of 5.27--5.38 dB, and correlation of 0.347--0.369 are far below values expected for reconstruction (typically PSNR $>$30 dB, correlation $>$0.9). The model generates plausible digits but not specific originals.

Diffusion models train via likelihood-based objectives \cite{ho2020denoisingdiffusion}, avoiding adversarial dynamics entirely. This provides stable training with monotonic loss improvement, eliminating mode collapse risk. However, inference requires 100 sequential denoising steps, making generation 115--230 times slower than GANs (153--297ms vs. 1.29ms per sample).

\subsection{Limitations}

Several limitations affect this study:

\textbf{Metrics:} Formal diversity metrics (Inception Score, FID) were not computed due to computational constraints. Visual inspection and proxy metrics (inter-sample variance, pairwise distance) provide evidence but lack the rigor of standard benchmarks.

\textbf{Dataset:} MNIST is simple compared to natural images. Mode collapse and instability may manifest differently on complex datasets like ImageNet or COCO.

\textbf{Architecture Search:} Only two DCGAN configurations were tested. A systematic hyperparameter sweep could identify stable configurations beyond the baseline.

\textbf{Diffusion Task Mismatch:} The diffusion model performs denoising rather than unconditional generation, complicating direct comparison. Testing an unconditional diffusion model would provide fairer evaluation.

\textbf{Training Duration:} Both GANs trained for 50 epochs. Longer training might reveal whether the modified model could eventually recover from collapse or whether the baseline would eventually degrade.

\textbf{Single Run:} Each configuration was trained once due to time constraints. Multiple runs with different seeds would better characterize variability and robustness.

% -------------------
\section{Conclusion}
Architectural decisions determine GAN training success. The baseline DCGAN maintains stable, diverse generation while the modified configuration collapses by epoch 35. Three factors cause this failure: reduced generator capacity (latent dimension 64 vs. 256), increased discriminator depth (4 vs. 3 layers), and dropout regularization (0.3). These create power imbalances where discriminators dominate, producing vanishing gradients. Diffusion models avoid adversarial instability through likelihood-based training but sacrifice inference speed (153--297ms vs. 1.29ms per sample).

Improving DCGAN stability requires balanced generator-discriminator capacity to prevent power imbalances. Several techniques address discriminator dominance. Spectral normalization \cite{miyato2018spectral} constrains weight matrices to prevent exploding gradients, allowing stable discriminator training without overwhelming the generator. Progressive growing \cite{karras2018progressive} begins training at low resolution (e.g., 4$\times$4) and gradually increases to target resolution, stabilizing feature learning at each scale. Lower discriminator learning rates (e.g., 0.0001 vs. 0.0002 for generator) slow discriminator adaptation, maintaining competitive dynamics \cite{salimans2016improved}.

For scaling to complex datasets, advanced architectures offer improvements. StyleGAN \cite{karras2019stylegan} introduces a mapping network that transforms latent codes before injection at multiple resolutions, separating high-level attributes (pose, identity) from fine details (texture, color), enabling controlled high-resolution synthesis (1024$\times$1024 and beyond). Latent diffusion models \cite{rombach2022latent} apply the diffusion process in a compressed latent space learned by an autoencoder rather than directly on pixels, reducing computational requirements by orders of magnitude while maintaining image quality. Hybrid approaches combine model families: training a diffusion model to learn high-quality distributions, then using knowledge distillation to transfer this capability into a GAN generator achieves diffusion-level quality with GAN-level inference speed.
% -------------------

\bibliographystyle{IEEEtran}
\bibliography{refs}

% -------------------
\onecolumn
\appendices
\section{Environment Details}

\subsection{Computing Infrastructure}
\begin{itemize}
    \item \textbf{HPC Cluster:} AI-Panther
    \item \textbf{Scheduler:} SLURM 22.05
    \item \textbf{GPU:} NVIDIA A100 40GB
    \item \textbf{CPU:} 8 cores per job
    \item \textbf{RAM:} 32GB per job
    \item \textbf{Partition:} gpu1
    \item \textbf{Time Limit:} 4 hours
\end{itemize}

\subsection{Container Configuration}
The Apptainer container (dcgan\_diffusion.sif) was built from the NVIDIA PyTorch base image (nvcr.io/nvidia/pytorch:25.09-py3) with the following specifications:

\begin{itemize}
    \item \textbf{Base Image:} NVIDIA NGC PyTorch 25.09-py3
    \item \textbf{Python:} 3.11
    \item \textbf{PyTorch:} 2.1.0+
    \item \textbf{CUDA:} 12.2
    \item \textbf{cuDNN:} Included with base image
    \item \textbf{Key Dependencies:}
    \begin{itemize}
        \item torchvision, torchmetrics, torch-fidelity
        \item diffusers, transformers, accelerate
        \item matplotlib, seaborn, pandas
        \item scikit-learn, scipy, pillow, tqdm
    \end{itemize}
    \item \textbf{Reproducibility Settings:}
    \begin{itemize}
        \item Random seed: 77 (fixed across all experiments)
        \item cuDNN deterministic: Enabled
        \item cuDNN benchmark: Disabled
        \item PYTHONUNBUFFERED: 1
    \end{itemize}
\end{itemize}

\subsection{SLURM Job Configuration}
\begin{verbatim}
#!/bin/bash
#SBATCH --job-name=dcgan_baseline
#SBATCH --partition=gpu1
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --time=04:00:00
#SBATCH --output=logs/%x_%j.out
#SBATCH --error=logs/%x_%j.err

module load apptainer/1.3.4-gcc-14.2.0-g7o5w4g
apptainer exec --nv container/dcgan_diffusion.sif \
    python python/train_dcgan.py \
    --config baseline \
    --epochs 50 \
    --seed 77
\end{verbatim}

\section{Additional Training Samples}

Figure~\ref{fig:baseline_evolution} shows the complete training evolution for the baseline DCGAN across all 50 epochs at 5-epoch intervals. The model progresses from random noise to high-quality, diverse digit generation.

\begin{figure*}[!h]
\centering
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_001.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_005.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_010.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_015.png}\\
\vspace{2mm}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_020.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_025.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_030.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_035.png}\\
\vspace{2mm}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_040.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_045.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_baseline/samples/epoch_050.png}
\caption{Baseline DCGAN training evolution at epochs 1, 5, 10, 15, 20, 25, 30, 35, 40, 45, and 50. Progressive improvement in sample quality and diversity is evident throughout training.}
\label{fig:baseline_evolution}
\end{figure*}

\clearpage
Figure~\ref{fig:modified_evolution} shows the training progression for the modified DCGAN. Mode collapse becomes evident by epoch 25 and completes by epoch 35.

\begin{figure*}[!h]
\centering
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_001.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_005.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_010.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_015.png}\\
\vspace{2mm}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_020.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_025.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_030.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_035.png}\\
\vspace{2mm}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_040.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_045.png}
\includegraphics[width=0.24\textwidth]{../../output/dcgan_modified/samples/epoch_050.png}
\caption{Modified DCGAN training evolution at epochs 1, 5, 10, 15, 20, 25, 30, 35, 40, 45, and 50. Mode collapse accelerates between epochs 20--35, resulting in identical grid patterns by epoch 35 that persist through epoch 50.}
\label{fig:modified_evolution}
\end{figure*}

\end{document}
